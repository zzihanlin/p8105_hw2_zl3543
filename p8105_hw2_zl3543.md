p8105_hw2_zl3543
================
Zihan Lin

``` r
# Problem 1
# Load necessary libraries
library(tidyverse)
```

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.4     ✔ readr     2.1.5
    ## ✔ forcats   1.0.0     ✔ stringr   1.5.1
    ## ✔ ggplot2   3.5.1     ✔ tibble    3.2.1
    ## ✔ lubridate 1.9.3     ✔ tidyr     1.3.1
    ## ✔ purrr     1.0.2     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

``` r
# Read the CSV data file
subway_data <- read_csv("/Users/suwa/Desktop/p8105_hw2_zl3543/data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv")
```

    ## Rows: 1868 Columns: 32
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (22): Division, Line, Station Name, Route1, Route2, Route3, Route4, Rout...
    ## dbl  (8): Station Latitude, Station Longitude, Route8, Route9, Route10, Rout...
    ## lgl  (2): ADA, Free Crossover
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
# Retain relevant columns and convert 'entry' to a logical variable
cleaned_data <- subway_data %>%
  select(line = `Line`, 
         station_name = `Station Name`, 
         station_latitude = `Station Latitude`, 
         station_longitude = `Station Longitude`, 
         routes = `Route1`:`Route11`, 
         entry = `Entry`, 
         vending = `Vending`, 
         entrance_type = `Entrance Type`, 
         ada_compliant = `ADA`) %>%
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))

# Check the dimensions of the cleaned dataset
dim(cleaned_data)
```

    ## [1] 1868   19

``` r
# The dataset contains information about entrances and exits for each subway station in NYC. Key variables include 'line', 'station_name', 'station_latitude', and 'station_longitude', 'routes', 'entry', 'vending', 'entrance_type', and 'ada_compliant'. For data cleaning, we retained only the relevant columns, renamed them for clarity, and converted the entry column from a character "YES"/"NO" to a logical 'TRUE'/'FALSE'. The resulting cleaned dataset has dimensions of [1868 rows x 19 columns], where row is the number of entries and column represents the selected variables. This dataset is tidy as each variable is in its own column, each observation is a row, and each cell contains a single value.

# Count distinct stations
distinct_stations <- cleaned_data %>%
  distinct(line, station_name)

num_distinct_stations <- nrow(distinct_stations)
num_distinct_stations
```

    ## [1] 465

``` r
# Include 'ada_compliant' in the distinct selection
distinct_stations <- cleaned_data %>%
  distinct(line, station_name, ada_compliant)

# Count ADA compliant stations
num_ada_compliant_stations <- distinct_stations %>%
  filter(ada_compliant == TRUE) %>%
  nrow()

num_ada_compliant_stations
```

    ## [1] 84

``` r
# Calculate the proportion of entrances/exits without vending that allow entry
no_vending_entries <- cleaned_data %>%
  filter(vending == "NO") %>%
  summarise(proportion = mean(entry, na.rm = TRUE))

no_vending_entries
```

    ## # A tibble: 1 × 1
    ##   proportion
    ##        <dbl>
    ## 1      0.377

``` r
# Convert all route columns to character to avoid type inconsistency
cleaned_data <- cleaned_data %>%
  mutate(across(starts_with("route"), as.character))

# Gather route columns into long format
route_data <- cleaned_data %>%
  pivot_longer(cols = starts_with("route"), 
               names_to = "route_number", 
               values_to = "route_name") %>%
  filter(!is.na(route_name))

# Display the first few rows to check the result
head(route_data)
```

    ## # A tibble: 6 × 10
    ##   line     station_name station_latitude station_longitude entry vending
    ##   <chr>    <chr>                   <dbl>             <dbl> <lgl> <chr>  
    ## 1 4 Avenue 25th St                  40.7             -74.0 TRUE  YES    
    ## 2 4 Avenue 25th St                  40.7             -74.0 TRUE  YES    
    ## 3 4 Avenue 36th St                  40.7             -74.0 TRUE  YES    
    ## 4 4 Avenue 36th St                  40.7             -74.0 TRUE  YES    
    ## 5 4 Avenue 36th St                  40.7             -74.0 TRUE  YES    
    ## 6 4 Avenue 36th St                  40.7             -74.0 TRUE  YES    
    ## # ℹ 4 more variables: entrance_type <chr>, ada_compliant <lgl>,
    ## #   route_number <chr>, route_name <chr>

``` r
# Count distinct stations serving the A train
stations_serving_A <- route_data %>%
  filter(route_name == "A") %>%
  distinct(line, station_name) 

num_stations_A <- nrow(stations_serving_A)
num_stations_A
```

    ## [1] 60

``` r
# Find the number of ADA compliant stations serving the A train
ada_compliant_A <- route_data %>%
  filter(route_name == "A", ada_compliant == "TRUE") %>%
  distinct(line, station_name) %>%
  nrow()

ada_compliant_A
```

    ## [1] 17

``` r
# Problem 2
# Load necessary libraries
library(tidyverse)
library(readxl)
library(janitor)
```

    ## 
    ## Attaching package: 'janitor'

    ## The following objects are masked from 'package:stats':
    ## 
    ##     chisq.test, fisher.test

``` r
# Read the Mr.Trash Wheel sheet, specifying the sheet and skipping non-data rows
mr_trash_wheel <- read_excel("/Users/suwa/Desktop/p8105_hw2_zl3543/data/202409 Trash Wheel Collection Data.xlsx", 
                             sheet = "Mr. Trash Wheel",
                             skip = 1) %>%
  # Rename columns to have reasonable variable names
  clean_names() %>%
  # Remove rows with missing dumpster-specific data
  filter(!is.na(dumpster)) %>%
  # Round the number of sports balls and convert to integer
  mutate(sports_balls = as.integer(round(sports_balls, 0)))
```

    ## New names:
    ## • `` -> `...15`
    ## • `` -> `...16`

``` r
# Read the Professor Trash Wheel sheet
prof_trash_wheel <- read_excel("/Users/suwa/Desktop/p8105_hw2_zl3543/data/202409 Trash Wheel Collection Data.xlsx", 
                               sheet = "Professor Trash Wheel", 
                               skip = 1) %>%
  clean_names() %>%
  filter(!is.na(dumpster)) %>%
  mutate(sports_balls = NA_integer_,  # Add sports_balls column as NA
         trash_wheel = "Professor Trash Wheel")

# Read and clean the Gwynnda Trash Wheel data
gwynnda_trash_wheel <- read_excel("/Users/suwa/Desktop/p8105_hw2_zl3543/data/202409 Trash Wheel Collection Data.xlsx", 
                                  sheet = "Gwynnda Trash Wheel", 
                                  skip = 1) %>%
  clean_names() %>%
  filter(!is.na(dumpster)) %>%
  mutate(sports_balls = NA_integer_,  # Add sports_balls column as NA
         trash_wheel = "Gwynnda Trash Wheel")

# Convert 'year' to numeric in all datasets
mr_trash_wheel <- mr_trash_wheel %>%
  mutate(year = as.numeric(year))

prof_trash_wheel <- prof_trash_wheel %>%
  mutate(year = as.numeric(year))

gwynnda_trash_wheel <- gwynnda_trash_wheel %>%
  mutate(year = as.numeric(year))

# Combine all datasets
combined_trash_wheel <- bind_rows(mr_trash_wheel, prof_trash_wheel, gwynnda_trash_wheel)

# Display the dimensions and the first few rows to check the result
dim(combined_trash_wheel)
```

    ## [1] 1033   17

``` r
head(combined_trash_wheel)
```

    ## # A tibble: 6 × 17
    ##   dumpster month  year date                weight_tons volume_cubic_yards
    ##      <dbl> <chr> <dbl> <dttm>                    <dbl>              <dbl>
    ## 1        1 May    2014 2014-05-16 00:00:00        4.31                 18
    ## 2        2 May    2014 2014-05-16 00:00:00        2.74                 13
    ## 3        3 May    2014 2014-05-16 00:00:00        3.45                 15
    ## 4        4 May    2014 2014-05-17 00:00:00        3.1                  15
    ## 5        5 May    2014 2014-05-17 00:00:00        4.06                 18
    ## 6        6 May    2014 2014-05-20 00:00:00        2.71                 13
    ## # ℹ 11 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <int>, homes_powered <dbl>, x15 <lgl>,
    ## #   x16 <lgl>, trash_wheel <chr>

``` r
# The process of cleaning data: 1) Importing Datasets: The data for the Great British Bake Off was spread across four CSV files: bakers.csv, bakes.csv, results.csv, and viewers.csv. Each dataset was imported using read_csv(), and column names were cleaned using clean_names() from the janitor package to standardize them to lowercase with underscores. 2) Handling the results Dataset: The results dataset did not have appropriate column names when imported. I reloaded the dataset using col_names = FALSE to read the first row as the actual column names. After setting the first row as the column names and removing that row, I cleaned the column names again using clean_names() to ensure consistency.I renamed relevant columns such as na_3 to baker, na_2 to episode, and na to series to maintain a consistent structure with other datasets. 3) Ensuring Data Type Consistency: I converted the baker columns in all datasets (bakers, results, bakes) to character types to avoid issues during joins. Additionally, the episode column was converted to numeric where necessary to ensure consistent data types. 4) Identifying and Handling Duplicates: Using group_by() and filter(n() > 1), I checked for duplicates in the results and bakes datasets. Duplicates were removed using distinct(), retaining only the first instance of each baker and episode combination. 5) Merging Datasets: The results, bakers, and bakes datasets were merged using left_join() to create a comprehensive dataset called combined_data. This dataset now contains information on each baker's background, their performance in each episode, and details of their bakes. 6) Final Adjustments: I renamed certain columns in combined_data to ensure clarity and consistency (e.g., series to season, baker_age to age, baker_occupation to occupation). I also reordered columns to make the dataset more intuitive, placing key identifiers like season, episode, baker, age, hometown, and occupation at the front. 7) Exporting the Cleaned Dataset: Finally, I exported the cleaned and combined dataset as combined_gbbo_data_cleaned.csv for further analysis.

# Questions and Choices Made: 1) Handling Missing or Ambiguous Data: When importing the results dataset, column names were missing or improperly structured. I chose to reload the data with col_names = FALSE to correctly capture the column headers. Additionally, I verified and renamed columns based on their content. 2) Data Type Conversions: I ensured consistent data types across datasets, particularly converting baker to character and episode to numeric, which was crucial for accurate data merging. 3) Handling Duplicates: I opted to keep the first instance of duplicates, but other methods (e.g., aggregation) could be considered if the data contains valuable repeated information.

# Final Dataset Description: The final combined_data dataset is a well-organized, comprehensive dataset that combines information about bakers, their bakes, and their performance in the Great British Bake Off across multiple episodes and seasons. It contains: 1) Key identifiers: season, episode, baker. 2) Demographic details: age, hometown, occupation. 3) Information about each baker's bakes and performance in the competition.
```

The combined dataset contains information on trash collection from three
different trash wheels: Mr. Trash Wheel, Professor Trash Wheel, and
Gwynnda. There are a total of 1033 observations and 17 variables in the
resulting dataset. Key variables include `dumpster`, which identifies
the dumpster number, `date` indicating when trash was collected,
`weight_tons` representing the weight of trash collected in tons,
`plastic_bottles` showing the number of plastic bottles collected, and
`sports_balls` counting the number of sports balls recovered. The total
weight of trash collected by Professor Trash Wheel is 246.74 tons.
Additionally, Gwynnda collected 0 cigarette butts in June 2022.

``` r
# Problem 3
# Load necessary libraries
library(tidyverse)
library(knitr)
library(janitor)

# Read the CSV data files
bakers <- read_csv("/Users/suwa/Desktop/p8105_hw2_zl3543/data/gbb_datasets/bakers.csv") %>% clean_names()
```

    ## Rows: 120 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker Name, Baker Occupation, Hometown
    ## dbl (2): Series, Baker Age
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
bakes <- read_csv("/Users/suwa/Desktop/p8105_hw2_zl3543/data/gbb_datasets/bakes.csv") %>% clean_names()
```

    ## Rows: 548 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker, Signature Bake, Show Stopper
    ## dbl (2): Series, Episode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
results <- read_csv("/Users/suwa/Desktop/p8105_hw2_zl3543/data/gbb_datasets/results.csv", col_names = FALSE)
```

    ## Rows: 1139 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (5): X1, X2, X3, X4, X5
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
# Set the first row of 'results' as column names and clean
colnames(results) <- results[1, ]
results <- results[-1, ] %>% clean_names()

# Rename relevant columns for consistency
results <- results %>%
  rename(
    baker = na_3,
    episode = na_2,
    series = na
  )

# Rename 'baker_name' to 'baker' if necessary
bakers <- bakers %>% rename(baker = baker_name)  # Adjust 'baker_name' to match the actual column name found in your dataset

# Convert 'baker' and 'episode' columns to appropriate data types
bakers <- bakers %>% mutate(baker = as.character(baker))
results <- results %>% mutate(baker = as.character(baker), episode = as.numeric(episode))
```

    ## Warning: There was 1 warning in `mutate()`.
    ## ℹ In argument: `episode = as.numeric(episode)`.
    ## Caused by warning:
    ## ! NAs introduced by coercion

``` r
bakes <- bakes %>% mutate(baker = as.character(baker), episode = as.numeric(episode))

# Check for discrepancies using anti_join (optional, useful for debugging)
bakes_without_bakers <- anti_join(bakes, bakers, by = "baker")
results_without_bakers <- anti_join(results, bakers, by = "baker")
results_without_bakes <- anti_join(results, bakes, by = c("baker", "episode"))

# Remove duplicates (keeping the first instance)
results <- results %>% distinct(baker, episode, .keep_all = TRUE)
bakes <- bakes %>% distinct(baker, episode, .keep_all = TRUE)

# Merge all datasets into combined_data
combined_data <- results %>%
  left_join(bakers, by = "baker") %>%
  left_join(bakes, by = c("baker", "episode"))

# Rename and reorder columns in combined_data for consistency and readability
combined_data <- combined_data %>%
  rename(
    season = series,
    age = baker_age,
    occupation = baker_occupation
  ) %>%
  select(season, episode, baker, age, hometown, occupation, everything())

# Inspect the final combined data
head(combined_data)
```

    ## # A tibble: 6 × 12
    ##   season episode baker       age hometown occupation series.x na_4     
    ##    <dbl>   <dbl> <chr>     <dbl> <chr>    <chr>      <chr>    <chr>    
    ## 1     NA      NA <NA>         NA <NA>     <NA>       <NA>     <NA>     
    ## 2     NA      NA baker        NA <NA>     <NA>       series   technical
    ## 3      1       1 Annetha      NA <NA>     <NA>       1        2        
    ## 4      1       1 David        NA <NA>     <NA>       1        3        
    ## 5      1       1 Edd          NA <NA>     <NA>       1        1        
    ## 6      1       1 Jasminder    NA <NA>     <NA>       1        <NA>     
    ## # ℹ 4 more variables:
    ## #   in_stayed_in_out_eliminated_star_baker_star_baker_winner_series_winner_runner_up_series_runner_up_wd_withdrew <chr>,
    ## #   series.y <dbl>, signature_bake <chr>, show_stopper <chr>

``` r
# Export the cleaned and combined dataset as a CSV
write_csv(combined_data, "combined_gbbo_data_cleaned.csv")

# Display column names of combined_data
colnames(combined_data)
```

    ##  [1] "season"                                                                                                       
    ##  [2] "episode"                                                                                                      
    ##  [3] "baker"                                                                                                        
    ##  [4] "age"                                                                                                          
    ##  [5] "hometown"                                                                                                     
    ##  [6] "occupation"                                                                                                   
    ##  [7] "series.x"                                                                                                     
    ##  [8] "na_4"                                                                                                         
    ##  [9] "in_stayed_in_out_eliminated_star_baker_star_baker_winner_series_winner_runner_up_series_runner_up_wd_withdrew"
    ## [10] "series.y"                                                                                                     
    ## [11] "signature_bake"                                                                                               
    ## [12] "show_stopper"

``` r
# Rename the column to 'result' if needed
combined_data <- combined_data %>%
  rename(result = in_stayed_in_out_eliminated_star_baker_star_baker_winner_series_winner_runner_up_series_runner_up_wd_withdrew)

# Check the first few rows of the combined_data to ensure data is present
head(combined_data)
```

    ## # A tibble: 6 × 12
    ##   season episode baker    age hometown occupation series.x na_4  result series.y
    ##    <dbl>   <dbl> <chr>  <dbl> <chr>    <chr>      <chr>    <chr> <chr>     <dbl>
    ## 1     NA      NA <NA>      NA <NA>     <NA>       <NA>     <NA>  <NA>         NA
    ## 2     NA      NA baker     NA <NA>     <NA>       series   tech… result       NA
    ## 3      1       1 Annet…    NA <NA>     <NA>       1        2     IN           NA
    ## 4      1       1 David     NA <NA>     <NA>       1        3     IN           NA
    ## 5      1       1 Edd       NA <NA>     <NA>       1        1     IN           NA
    ## 6      1       1 Jasmi…    NA <NA>     <NA>       1        <NA>  IN           NA
    ## # ℹ 2 more variables: signature_bake <chr>, show_stopper <chr>

``` r
# Display unique values in the 'result' column
unique(combined_data$result)
```

    ## [1] NA           "result"     "IN"         "OUT"        "Runner-up" 
    ## [6] "WINNER"     "STAR BAKER" "WD"         "[a]"

``` r
# Check how many rows match the criteria
filtered_data <- combined_data %>%
  filter(season >= 5 & season <= 10, result %in% c("Star Baker", "Winner"))

nrow(filtered_data)
```

    ## [1] 0

``` r
# Clean the result column by trimming whitespace and converting to a consistent case
combined_data <- combined_data %>%
  mutate(result = str_trim(result) %>% str_to_title())

# Check unique values again to confirm they are cleaned
unique(combined_data$result)
```

    ## [1] NA           "Result"     "In"         "Out"        "Runner-Up" 
    ## [6] "Winner"     "Star Baker" "Wd"         "[A]"

``` r
# Filter and create the Star Baker/Winner table for Seasons 5 to 10
star_baker_table <- combined_data %>%
  filter(season >= 5 & season <= 10, result %in% c("Star Baker", "Winner")) %>%
  select(season, episode, baker, result) %>%
  arrange(season, episode)

# Display the table
knitr::kable(
  star_baker_table, 
  caption = "Star Baker/Winner for Each Episode (Seasons 5 to 10)",
  col.names = c("Season", "Episode", "Baker", "Result"),
  format = "markdown"
)
```

| Season | Episode | Baker     | Result     |
|-------:|--------:|:----------|:-----------|
|      5 |       1 | Nancy     | Star Baker |
|      5 |       2 | Richard   | Star Baker |
|      5 |       3 | Luis      | Star Baker |
|      5 |       4 | Richard   | Star Baker |
|      5 |       5 | Kate      | Star Baker |
|      5 |       6 | Chetna    | Star Baker |
|      5 |       7 | Richard   | Star Baker |
|      5 |       8 | Richard   | Star Baker |
|      5 |       9 | Richard   | Star Baker |
|      5 |      10 | Nancy     | Winner     |
|      6 |       1 | Marie     | Star Baker |
|      6 |       5 | Nadiya    | Star Baker |
|      6 |       6 | Mat       | Star Baker |
|      6 |       7 | Tamal     | Star Baker |
|      6 |       8 | Nadiya    | Star Baker |
|      6 |       9 | Nadiya    | Star Baker |
|      6 |      10 | Nadiya    | Winner     |
|      7 |       1 | Jane      | Star Baker |
|      7 |       2 | Candice   | Star Baker |
|      7 |       3 | Tom       | Star Baker |
|      7 |       4 | Benjamina | Star Baker |
|      7 |       5 | Candice   | Star Baker |
|      7 |       6 | Tom       | Star Baker |
|      7 |       7 | Andrew    | Star Baker |
|      7 |       8 | Candice   | Star Baker |
|      7 |       9 | Andrew    | Star Baker |
|      7 |      10 | Candice   | Winner     |
|      8 |       1 | Steven    | Star Baker |
|      8 |       2 | Steven    | Star Baker |
|      8 |       3 | Julia     | Star Baker |
|      8 |       5 | Sophie    | Star Baker |
|      8 |       6 | Liam      | Star Baker |
|      8 |       7 | Steven    | Star Baker |
|      8 |       8 | Stacey    | Star Baker |
|      8 |       9 | Sophie    | Star Baker |
|      8 |      10 | Sophie    | Winner     |

Star Baker/Winner for Each Episode (Seasons 5 to 10)

``` r
# Save the table to a CSV
write_csv(star_baker_table, "star_baker_winner_seasons5_10.csv")

# Read the viewership data
viewers <- read_csv("/Users/suwa/Desktop/p8105_hw2_zl3543/data/gbb_datasets/viewers.csv") %>% clean_names()
```

    ## Rows: 10 Columns: 11
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (11): Episode, Series 1, Series 2, Series 3, Series 4, Series 5, Series ...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
# Display the first 10 rows of the viewership data
head(viewers, 10)
```

    ## # A tibble: 10 × 11
    ##    episode series_1 series_2 series_3 series_4 series_5 series_6 series_7
    ##      <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>
    ##  1       1     2.24     3.1      3.85     6.6      8.51     11.6     13.6
    ##  2       2     3        3.53     4.6      6.65     8.79     11.6     13.4
    ##  3       3     3        3.82     4.53     7.17     9.28     12.0     13.0
    ##  4       4     2.6      3.6      4.71     6.82    10.2      12.4     13.3
    ##  5       5     3.03     3.83     4.61     6.95     9.95     12.4     13.1
    ##  6       6     2.75     4.25     4.82     7.32    10.1      12       13.1
    ##  7       7    NA        4.42     5.1      7.76    10.3      12.4     13.4
    ##  8       8    NA        5.06     5.35     7.41     9.02     11.1     13.3
    ##  9       9    NA       NA        5.7      7.41    10.7      12.6     13.4
    ## 10      10    NA       NA        6.74     9.45    13.5      15.0     15.9
    ## # ℹ 3 more variables: series_8 <dbl>, series_9 <dbl>, series_10 <dbl>

``` r
# Reshape the dataset from wide to long format
viewers_long <- viewers %>%
  pivot_longer(
    cols = starts_with("series"),  # Pivot all columns that start with "series"
    names_to = "season",           # Create a "season" column
    values_to = "viewership"       # Place the values into a "viewership" column
  ) %>%
  mutate(
    season = as.numeric(str_replace(season, "series_", ""))  # Convert "series_1", "series_2", etc. to numeric
  )

# Check the structure of the reshaped data
head(viewers_long)
```

    ## # A tibble: 6 × 3
    ##   episode season viewership
    ##     <dbl>  <dbl>      <dbl>
    ## 1       1      1       2.24
    ## 2       1      2       3.1 
    ## 3       1      3       3.85
    ## 4       1      4       6.6 
    ## 5       1      5       8.51
    ## 6       1      6      11.6

``` r
# Save the viewership data
write_csv(viewers_long, "viewership_data_cleaned.csv")

# Calculate average viewership for Season 1
avg_viewership_s1 <- viewers_long %>%
  filter(season == 1) %>%
  summarise(average_viewership = mean(viewership, na.rm = TRUE))

# Calculate average viewership for Season 5
avg_viewership_s5 <- viewers_long %>%
  filter(season == 5) %>%
  summarise(average_viewership = mean(viewership, na.rm = TRUE))

# Display the results
avg_viewership_s1$average_viewership
```

    ## [1] 2.77

``` r
avg_viewership_s5$average_viewership
```

    ## [1] 10.0393
